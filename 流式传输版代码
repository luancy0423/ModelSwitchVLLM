# ===== 1. 环境安装 =====
!pip install -q bitsandbytes  # 安装缺少的依赖
!pip install -q transformers accelerate torchvision datasets matplotlib opencv-python
!pip install -q git+https://github.com/huggingface/transformers.git

# ===== 2. 导入库 =====
import torch
import numpy as np
import requests
from PIL import Image
import matplotlib.pyplot as plt
from transformers import (
    Blip2Processor, Blip2ForConditionalGeneration,
    OwlViTProcessor, OwlViTForObjectDetection,
    AutoProcessor
)
from datasets import load_dataset, Dataset
import cv2
from io import BytesIO
import os
import logging
from tqdm import tqdm

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===== 3. 模型初始化（简化版本）=====
device = "cuda" if torch.cuda.is_available() else "cpu"

# 设置缓存目录为内存中的tmpfs（如果可用）
cache_dir = "/dev/shm/hf_cache" if os.path.exists("/dev/shm") else None
if cache_dir and not os.path.exists(cache_dir):
    os.makedirs(cache_dir)
    logger.info(f"创建内存缓存目录: {cache_dir}")

# 1. BLIP-2 (轻量多模态) - 不使用8位量化
blip_processor = Blip2Processor.from_pretrained(
    "Salesforce/blip2-opt-2.7b", 
    cache_dir=cache_dir
)
blip_model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b", 
    torch_dtype=torch.float16 if device=="cuda" else torch.float32,
    cache_dir=cache_dir
).eval().to(device)

# 2. OWL-ViT (视觉定位)
owl_processor = OwlViTProcessor.from_pretrained(
    "google/owlvit-base-patch32",
    cache_dir=cache_dir
)
owl_model = OwlViTForObjectDetection.from_pretrained(
    "google/owlvit-base-patch32", 
    torch_dtype=torch.float16 if device=="cuda" else torch.float32,
    cache_dir=cache_dir
).eval().to(device)

logger.info("✅ 模型加载完成")

# ===== 4. 动态切换类（修改版）=====
class VLModelSwitch:
    def __init__(self, models):
        self.models = models
    
    def _blip_inference(self, image, query, num_samples=3):
        """BLIP2多采样推理"""
        inputs = self.models['blip']['processor'](images=image, text=query, return_tensors="pt").to(device)
        outputs = []
        for _ in range(num_samples):
            generated_ids = self.models['blip']['model'].generate(**inputs, max_new_tokens=50)
            answer = self.models['blip']['processor'].batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
            outputs.append(answer)
        return outputs

    def _calc_consistency(self, answers):
        """计算答案Jaccard相似度作为一致性指标"""
        if len(answers) < 2:
            return 1.0  # 单样本默认高一致
        
        scores = []
        for i in range(len(answers)):
            for j in range(i+1, len(answers)):
                set_i = set(answers[i].lower().split())
                set_j = set(answers[j].lower().split())
                inter = len(set_i & set_j)
                union = max(len(set_i | set_j), 1)  # 避免除0
                scores.append(inter / union)
        return np.mean(scores) if scores else 0.0

    def switch(self, image, query, k=5, tau=0.7):
        """核心切换逻辑"""
        # 阶段1: BLIP-2初始采样
        blip_outputs = self._blip_inference(image, query, num_samples=k//2 + 1)
        consistency = self._calc_consistency(blip_outputs)
        
        # 阶段2: 一致性判断
        if consistency >= tau:
            # 高一致性直接投票
            final_answer = max(set(blip_outputs), key=blip_outputs.count)
            used_models = ["BLIP2"]
            used_samples = len(blip_outputs)
        else:
            # 低一致性切换模型
            if "where" in query.lower() or "locate" in query.lower():
                # 定位问题 → OWL-ViT
                inputs = self.models['owl']['processor'](
                    text=[[query]], images=image, return_tensors="pt"
                ).to(device)
                outputs = self.models['owl']['model'](**inputs)
                
                # 获取图像尺寸
                if isinstance(image, Image.Image):
                    width, height = image.size
                else:
                    height, width = image.shape[:2]
                
                # 后处理检测结果
                target_sizes = torch.tensor([[height, width]])
                results = self.models['owl']['processor'].post_process_object_detection(
                    outputs, threshold=0.2, target_sizes=target_sizes
                )[0]
                
                # 格式化输出
                boxes = results["boxes"].cpu().detach().numpy().tolist()
                final_answer = {"detected_objects": boxes}
                used_models = ["BLIP2", "OWL-ViT"]
                used_samples = k//2 + 1
            else:
                # 推理问题 → 使用BLIP再次采样（原LLaVA替代方案）
                additional_outputs = self._blip_inference(image, query, num_samples=k//2 + 1)
                all_outputs = blip_outputs + additional_outputs
                final_answer = max(set(all_outputs), key=all_outputs.count)
                used_models = ["BLIP2"]
                used_samples = len(all_outputs)
        
        return final_answer, used_models, used_samples, consistency

# ===== 5. 初始化ModelSwitch =====
models_dict = {
    "blip": {"model": blip_model, "processor": blip_processor},
    "owl": {"model": owl_model, "processor": owl_processor}
}
vlm_switch = VLModelSwitch(models_dict)

# ===== 6. 流式加载测试数据 =====
# 下载示例图像（带重试和备用）
def download_image(url, max_retries=2):
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return Image.open(BytesIO(response.content))
        except Exception as e:
            logger.warning(f"下载图像失败（尝试{attempt+1}/{max_retries}）: {str(e)}")
    # 备用：返回一张空白图像
    logger.info("使用备用图像")
    return Image.new('RGB', (100, 100), color='red')

# 更新为新的图像URL
simple_image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/1200px-Cat_November_2010-1a.jpg"
hard_image_url = "https://huggingface.co/datasets/mishig/sample_images/resolve/main/coco_sample.png"

# 示例1: 简单问题（高一致性）
simple_image = download_image(simple_image_url)
simple_query = "这是什么动物？"

# 示例2: 困难问题（需切换）
hard_image = download_image(hard_image_url)
hard_query = "图中食物左侧的物体是什么？"

# 流式加载VQA验证集（使用流式传输减少磁盘占用）
logger.info("流式加载VQA验证集...")
try:
    # 使用streaming=True参数启用流式传输
    vqa_stream = load_dataset("HuggingFaceM4/VQAv2", split="validation", streaming=True)
    
    # 创建内存中的小数据集（仅处理前50个样本）
    vqa_val = []
    sample_count = 0
    max_samples = 50
    
    # 使用进度条
    pbar = tqdm(total=max_samples, desc="加载样本")
    
    for sample in vqa_stream:
        if sample_count >= max_samples:
            break
            
        try:
            # 流式传输时，图像是bytes类型，需要转换为PIL图像
            if isinstance(sample["image"], bytes):
                image = Image.open(BytesIO(sample["image"]))
            else:
                image = sample["image"]
                
            vqa_val.append({
                "image": image,
                "question": sample["question"],
                "answers": sample["answers"]
            })
            sample_count += 1
            pbar.update(1)
        except Exception as e:
            logger.error(f"处理样本出错: {str(e)}")
    
    pbar.close()
    logger.info(f"成功加载 {len(vqa_val)} 个样本")
    
except Exception as e:
    # 备用方案：手动创建小数据集
    logger.error(f"数据集加载失败 ({e})，使用备份样本")
    vqa_val = [
        {"image": simple_image, "question": "这是什么动物？", "answers": {"text": ["猫"]}},
        {"image": hard_image, "question": "图中食物左侧的物体是什么？", "answers": {"text": ["叉子"]}}
    ]

# ===== 7. 评估函数 =====
def evaluate_switch(model_switch, dataset, tau=0.7):
    results = []
    pbar = tqdm(dataset, desc="评估样本")
    
    for item in pbar:
        # 确保图像是PIL格式
        image = item["image"]
        if not isinstance(image, Image.Image):
            try:
                # 如果图像是数组格式则转换
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image.astype('uint8'))
                # 如果图像是张量则转换
                elif torch.is_tensor(image):
                    image = Image.fromarray(image.numpy().astype('uint8'))
                else:
                    # 尝试强制转换为数组再转PIL
                    image = Image.fromarray(np.array(image).astype('uint8'))
            except Exception as e:
                logger.error(f"无法将数据集图像转换为PIL Image: {e}")
                continue # Skip this item if conversion fails

        query = item["question"]
        # Ensure answers is a list of strings
        answers = item["answers"]["text"] if "text" in item["answers"] else []
        if not isinstance(answers, list):
             answers = [str(answers)] # Convert single answer to list

        # 运行ModelSwitch
        try:
            pred, models_used, samples_used, consistency = model_switch.switch(
                image, query, tau=tau
            )
        except Exception as e:
            logger.error(f"Switch inference failed for query '{query}': {e}")
            continue # Skip item on inference failure

        # 简化评估：预测是否匹配任一参考答案
        correct = 0
        if isinstance(pred, str):  # 文本答案
            pred_clean = pred.lower().strip()
            for ans in answers:
                if ans and (ans.lower() in pred_clean or pred_clean in ans.lower()): # Add check for empty answer
                    correct = 1
                    break
        elif isinstance(pred, dict) and 'detected_objects' in pred:  # 物体检测结果
            # 简化验证：只要检测到物体就算正确
            correct = 1 if pred.get('detected_objects') else 0
        else:
            logger.warning(f"Unexpected prediction format for query '{query}': {type(pred)}")

        results.append({
            "query": query,
            "pred": str(pred)[:100],  # 截断长输出
            "correct": correct,
            "models_used": models_used,
            "samples_used": samples_used,
            "consistency": consistency
        })
        
        # 更新进度条描述
        pbar.set_description(f"评估样本 ({len(results)}/{len(dataset)})")
    
    pbar.close()
    return results

# ===== 8. 运行实验 =====
# 测试简单样本
logger.info("运行简单样本测试...")
try:
    simple_result, _, _, _ = vlm_switch.switch(simple_image, simple_query)
    logger.info(f"简单问题结果: {simple_result}")
except Exception as e:
    logger.error(f"简单问题测试失败: {str(e)}")

# 测试困难样本
logger.info("运行困难样本测试...")
try:
    hard_result, models_used, samples, consistency = vlm_switch.switch(hard_image, hard_query)
    logger.info(f"困难问题结果: {hard_result} | 使用模型: {models_used} | 采样数: {samples} | 一致性: {consistency:.2f}")
except Exception as e:
    logger.error(f"困难问题测试失败: {str(e)}")

# 评估样本
logger.info("评估样本...")
try:
    results = evaluate_switch(vlm_switch, vqa_val)
    logger.info(f"成功评估 {len(results)} 个样本")
except Exception as e:
    logger.error(f"评估失败: {str(e)}")
    results = []

# ===== 9. 性能分析 =====
if results:
    # 计算准确率
    accuracy = sum(item['correct'] for item in results) / len(results)
    # 计算平均采样数
    avg_samples = sum(item['samples_used'] for item in results) / len(results)
    # 统计切换比例
    switch_rate = sum(1 for item in results if len(item['models_used']) > 1) / len(results)

    print(f"\n=== 性能报告 ===")
    print(f"样本数: {len(results)}")
    print(f"准确率: {accuracy:.2%}")
    print(f"平均采样数: {avg_samples:.1f}")
    print(f"模型切换比例: {switch_rate:.1%}")

    # 可视化一致性分布
    if len(results) > 5:
        plt.figure(figsize=(10, 6))
        consistencies = [item['consistency'] for item in results]
        plt.hist(consistencies, bins=10, alpha=0.7, color='skyblue')
        plt.title("多模态一致性分布")
        plt.xlabel("一致性分数")
        plt.ylabel("样本数")
        plt.grid(True, linestyle='--', alpha=0.5)
        plt.savefig('consistency_distribution.png', bbox_inches='tight')
        plt.close()
        print("一致性分布图已保存为 consistency_distribution.png")
else:
    print("无有效结果可分析")

# ===== 10. 清理资源 =====
logger.info("清理资源...")
del blip_model, owl_model
torch.cuda.empty_cache()

# 清理缓存
if cache_dir and os.path.exists(cache_dir):
    !rm -rf {cache_dir}
    logger.info(f"清理缓存目录: {cache_dir}")

logger.info("测试完成!")
